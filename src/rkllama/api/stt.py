import numpy as np
import soundfile as sf
import logging
from typing import Tuple
import os
import io
from rknnlite.api import RKNNLite
import soxr

logger = logging.getLogger("rkllama.stt")

# GLobal constants
SAMPLE_RATE = 16000 # Allowed by the original model OmniASR
BLANK_ID = 0

class STTModelRKNN:
    def __init__(
        self,
        model_path: str,
    ):

        # Get encoder, decoder and config file from models path
        model_name, tokens_file = find_model_files(model_path)

        # Prepare the RKNN runtime model
        self.model_rknn = RKNNLite(verbose=False)
        self.model_rknn.load_rknn(model_name)
        self.model_rknn.init_runtime(core_mask=RKNNLite.NPU_CORE_AUTO)

        # Save the tokens file
        self.tokens = self.load_tokens(tokens_file)

    def __call__(self, x: np.ndarray, chunk_len: int):
        
        logger.debug(f"Original input shape received: {x.shape}")
        # Padding to match RKNN model input
        padded_x = np.zeros((x.shape[0], chunk_len), dtype=np.float32)
        padded_x[:, :x.shape[1]] = x
        logger.debug(f"Padded input shape: {padded_x.shape}")
        
        # RUn RKNN inference
        logits = self.model_rknn.inference(inputs=[padded_x], data_format="nchw")[0]
        logger.debug(f"Logits generated by RKNN: {logits.shape}")
        
        # Remove the extra zeros added to the input
        logits = self.trim_rknn_asr_output( logits, real_input_len=x.shape[1],  static_input_len=padded_x.shape[1])
        logger. debug(f"Logits after trim: {logits.shape}")
        
        # Return expected output
        return logits


    def trim_rknn_asr_output(self, result, real_input_len, static_input_len):
        """
        Trim the original RKNN output in the required one without extra tokens: np.ndarray shape (1, T, vocab)
        """
        # Get the actual shape
        B, T, V = result.shape

        # Reshape to the expected output shape
        T_real = int(round(T * real_input_len / static_input_len))

        # Return the real output
        return result[:, :T_real, :]


    def load_tokens(self, tokens_file_path):
        """
        Load ll the tokens for the model tokenizer
        
        Argss:
        
            tokens_file_path (str): Tokens file
        """
        id2token = dict()
        with open(tokens_file_path, encoding="utf-8") as f:
            for line in f:
                fields = line.split()
                if len(fields) == 1:
                    id2token[int(fields[0])] = " "
                else:
                    t, idx = fields
                    id2token[int(idx)] = t
        return id2token
    

    def load_audio_from_bytes(self, data: bytes) -> np.ndarray:
        """
        Load an audio from bytes
        
        Args:
            data (str): Bytes of the audio file
        Returns:
            np.ndarray: Audio in this format
        """
        # Read the audio
        audio, sr = sf.read(io.BytesIO(data), dtype="float32")
        logger.debug(f"Original SR from audio file: {sr}")

        # Only use the first channel
        if audio.ndim > 1:
            audio = audio[:, 0] 

        # Adjust the SR if the original audio not in that size
        if sr != SAMPLE_RATE:
            logger.debug(f"Resample audio file to {SAMPLE_RATE}")
            audio = soxr.resample(audio, sr, SAMPLE_RATE)

        # GLobal Normalization
        mean = np.mean(audio)
        var = np.var(audio)
        audio = (audio - mean) / np.sqrt(var + 1e-5)

        # Return the audio
        return audio

    def get_best_chunk_size_for_input(self, samples):
        """
        Return the bestchunk size for most efficiency by RKNN dynamic input
        
        Args:
            samples: Samples of the audio to transcription
        """
        # Define the list of allowed sized used in convertion of ONNX to RKNN model
        allowed_chunk_sizes = [5, 10, 20, 30, 40]

        # Loop over the allowed chunk sizes by rknn to fit the better one for the samples to prevent multiple chunks
        for size in allowed_chunk_sizes:
            if len(samples) <= (size * SAMPLE_RATE):
                return size

        # Default chunk size. Will generate multiples chunks
        return 10
    

    def split_audio(self, samples, overlap_sec=1.0):
        """
        Split the samples in chunks for processing
        
        Args:
            samples: Audio samples
            overlap_sec (flotat): Second to overlap between chunks
        Returns:
           Array: Array of audio chunks
        """
        
        # Get the best fit for RKNN input by the current samples
        chunk_sec = self.get_best_chunk_size_for_input(samples)
        logger.debug(f"chunk_sec: {chunk_sec}")


        # Calculate values for the split
        chunk_len = int(chunk_sec * SAMPLE_RATE)
        overlap = int(overlap_sec * SAMPLE_RATE)
        step = chunk_len - overlap
        logger.debug(f"chunk_len: {chunk_len} | overlap: {overlap} | step: {step}  | samples: {len(samples)} ")

        chunks = []
        pos = 0
        # Create the audio chunks
        while pos < len(samples):
            chunks.append(samples[pos : pos + chunk_len])
            pos += step

        # Return the chunks
        logger.debug(f"Number of generated chunks: {len(chunks)}")
        return chunks, chunk_len


    def ctc_decode(self, ids):
        """
        Apply the CTC component of the model
        
        """
        out = []
        prev = -1
        for i in ids:
            if i != BLANK_ID and i != prev:
                out.append(i)
            prev = i
        return out


    

def generate_transcription(model_omniasr_path,file,language) -> str:
    """
    Generate a transcription
    
    model_omniasr_path (str): Path of the omniASR model
    file (file): Audio file to trancribe
    language: Language of the text

    Returns:
        str: Transcription text
    """
    
    # Create an instance of the model
    model = STTModelRKNN(model_omniasr_path)

    # read the bytes from the audio file
    audio_bytes = file.read()

    # Get the samples of the auido
    samples = model.load_audio_from_bytes(audio_bytes)

    # Split the audio in chunks
    chunks, chunks_len = model.split_audio(samples)

    # Call the model for every chunk
    all_ids = []
    for chunk in chunks:

        # Call the encoder model
        logits = model(chunk[None], chunks_len)
        
        # Get the token output from the model
        ids = logits[0].argmax(axis=-1)

        # Remove the blanks tokens from the output
        decoded = model.ctc_decode(ids)

        # Remove duplicates between chunks
        if all_ids and decoded and all_ids[-1] == decoded[0]:
            decoded = decoded[1:]

        # Add the token to the final list
        all_ids.extend(decoded)

    # Release resources from RKNN
    model.model_rknn.release()

    # Loop over the token list and search in vocabulary for the mapping characters
    transcription = "".join(model.tokens[i] for i in all_ids)

    # Return the transcription
    return transcription



def find_model_files(dir_path: str) -> Tuple[str, str]:
    """
    Find exactly one .rknn and .txt file in a omniasr model directory.

    Returns:
        (rknn_path, txt_path)

    Raises:
        FileNotFoundError: if a required file type is missing
        ValueError: if more than one file of a type is found
    """

    if not os.path.isdir(dir_path):
        raise NotADirectoryError(f"Not a directory: {dir_path}")

    model_rknn = None
    tokens_txt = None

    for name in os.listdir(dir_path):
        path = os.path.join(dir_path, name)
        if not os.path.isfile(path):
            # Skip dictionaries
            continue

        # Check the current file in the directoty loop
        ext = os.path.splitext(name)[1].lower()
        if ext == ".rknn":
            if not model_rknn:
                model_rknn = path
        elif ext == ".txt":
            if not tokens_txt:
                tokens_txt = path
        
        if model_rknn and tokens_txt:
            # All files found. Exit loop
            break

    # Return the files    
    return ( model_rknn, tokens_txt)